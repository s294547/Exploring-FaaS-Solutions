grafana:
  global:
    domain_name: 'liquidfaas.cloud'
  opt:
    image: "grafana/grafana:9.4.7"
    log_level: warn
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 250m
        memory: 384Mi
  plugins:
    - grafana-piechart-panel
    - petrslavotinek-carpetplot-panel
    - grafana-clock-panel
    - snuids-trafficlights-panel
  auth:
    admin:
      username: admin
      password: admin
    oidc:
      enabled: false
  influx:
    enabled: true
    token: ZwWM2Gx17aMnJrBSIvbec1WK0Xga1oCJ
    organization: influxdata
    bucket: aggregates
    url: http://openwhisk-influxdb2:80
  dashboard:
    enabled: true
  grafanauser:
    enabled: true
    name: external
    password: external
grafana-operator:
  operator:
    image: "quay.io/grafana-operator/grafana-operator:v4.10.0"
    resources:
      requests:
        cpu: 20m
        memory: 32Mi
      limits:
        cpu: 50m
        memory: 64Mi
openwhisk:
  whisk:
    ingress:
      type: NodePort
      apiHostName: 38.242.158.232
      apiHostPort: 30000
  nginx:
    httpsNodePort: 30000
  auth:
    system: "789c46b1-71f6-4ed5-8c54-816aa4f8c502:abczO3xZCLrMN6v2BKK1dXYFpXlPkccOFqm12CdAsMgRU4VrNZ9lyGVCGuMDGIwP"
    guest: 23bc46b1-71f6-4ed5-8c54-816aa4f8c502:123zO3xZCLrMN6v2BKK1dXYFpXlPkccOFqm12CdAsMgRU4VrNZ9lyGVCGuMDGIwP
  systemNameSpace: "/whisk.system"
  # Database configuration

  # Database configuration
  db:
    external: true
    host: openwhisk-svc-couchdb
    protocol: "http"
    auth:
      username: admin
      password: admin
    # Should we run a Job to wipe and re-initialize the database when the chart is deployed?

    # This should always be true if external is false.
    wipeAndInit: true
    port: 5984
  providers:
    # Kafka provider configurations
    kafka:
      enabled: false
  probes:
    zookeeper:
      livenessProbe:
        initialDelaySeconds: 100
        periodSeconds: 10
        timeoutSeconds: 10000
      readinessProbe:
        initialDelaySeconds: 100
        periodSeconds: 10
        timeoutSeconds: 10000
    controller:
      livenessProbe:
        initialDelaySeconds: 200
        periodSeconds: 100
        timeoutSeconds: 10000
      readinessProbe:
        initialDelaySeconds: 200
        periodSeconds: 100
        timeoutSeconds: 10000
    kafka:
      livenessProbe:
        initialDelaySeconds: 100
        periodSeconds: 10
        timeoutSeconds: 10000
      readinessProbe:
        initialDelaySeconds: 100
        periodSeconds: 10
        timeoutSeconds: 10000
    invoker:
      livenessProbe:
        initialDelaySeconds: 100
        periodSeconds: 10
        timeoutSeconds: 10000
      readinessProbe:
        initialDelaySeconds: 100
        periodSeconds: 10
        timeoutSeconds: 10000
influxdb2:
  image:
    repository: influxdb
    tag: 2.3.0-alpine
    pullPolicy: IfNotPresent
    ## If specified, use these secrets to access the images
  # pullSecrets:

  #   - registry-secret

  ## Annotations to be added to InfluxDB pods

  ##
  podAnnotations: {}
  ## Labels to be added to InfluxDB pods

  ##
  podLabels:
    name: influxdb
    user-action-pod: "true"
  nameOverride: ""
  fullnameOverride: ""
  ## Configure resource requests and limits

  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/

  ##
  resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious

  # choice for the user. This also increases chances charts run on environments with little

  # resources, such as Minikube. If you do want to specify resources, uncomment the following

  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.

  # limits:

  #  cpu: 100m

  #  memory: 128Mi

  # requests:

  #  cpu: 100m

  #  memory: 128Mi

  ## Node labels for pod assignment

  ## ref: https://kubernetes.io/docs/user-guide/node-selection/

  ##
  nodeSelector: {}
  ## Tolerations for pod assignment

  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/

  ##
  tolerations: []
  ## Affinity for pod assignment

  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity

  ##
  affinity: {}
  securityContext: {}
  ## Customize liveness, readiness and startup probes

  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/

  ##
  livenessProbe:
    path: "/health"
    scheme: "HTTP"
    initialDelaySeconds: 60
    periodSeconds: 10
    timeoutSeconds: 100
    failureThreshold: 1000
  readinessProbe:
    path: "/health"
    scheme: "HTTP"
    initialDelaySeconds: 60
    periodSeconds: 10
    timeoutSeconds: 100
    failureThreshold: 1000
    successThreshold: 1
  startupProbe:
    enabled: false
    # path: "/health"
  # scheme: "HTTP"

  # initialDelaySeconds: 30

  # periodSeconds: 5

  # timeoutSeconds: 1

  # failureThreshold: 6

  ## Extra environment variables to configure influxdb

  ## e.g.

  # env:

  #   - name: FOO

  #     value: BAR

  #   - name: BAZ

  #     valueFrom:

  #       secretKeyRef:

  #         name: my-secret

  #         key: my-key
  env: {}
  ## Create default user through docker entrypoint

  ## Defaults indicated below

  ##
  adminUser:
    organization: influxdata
    bucket: aggregates
    user: admin
    retention_policy: "0s"
    ## Leave empty to generate a random password and token.

    ## Or fill any of these values to use fixed values.
    password: adminopenwhiskinfluxdb
    token: ZwWM2Gx17aMnJrBSIvbec1WK0Xga1oCJ
    ## The password and token are obtained from an existing secret. The expected
  ## keys are `admin-password` and `admin-token`.

  ## If set, the password and token values above are ignored.

  # existingSecret: influxdb-auth

  ## Persist data to a persistent volume

  ##
  persistence:
    enabled: true
    ## If true will use an existing PVC instead of creating one

    # useExisting: false

    ## Name of existing PVC to be used in the influx deployment

    # name:

    ## influxdb data Persistent Volume Storage Class

    ## If defined, storageClassName: <storageClass>

    ## If set to "-", storageClassName: "", which disables dynamic provisioning

    ## If undefined (the default) or set to null, no storageClassName spec is

    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on

    ##   GKE, AWS & OpenStack)

    ##

    # storageClass: "-"
    accessMode: ReadWriteOnce
    size: 50Gi
    mountPath: /var/lib/influxdb2
    subPath: ""
  ## Add custom volume and volumeMounts

  ##

  # volumes:

  #   - name: influxdb2-templates

  #     hostPath:

  #       path: /data/influxdb2-templates

  #       type: Directory

  # mountPoints:

  #   - name: influxdb2-templates

  #     mountPath: /influxdb2-templates

  #     readOnly: true

  ## Allow executing custom init scripts

  ## If the container finds any files with the .sh extension inside of the

  ## /docker-entrypoint-initdb.d folder, it will execute them.

  ## When multiple scripts are present, they will be executed in lexical sort order by name.

  ## For more details see Custom Initialization Scripts in https://hub.docker.com/_/influxdb
  initScripts:
    enabled: false
    scripts:
      init.sh: |+
        #!/bin/bash
        influx apply --force yes -u https://raw.githubusercontent.com/influxdata/community-templates/master/influxdb2_operational_monitoring/influxdb2_operational_monitoring.yml

  ## Specify a service type

  ## ref: http://kubernetes.io/docs/user-guide/services/

  ##
  service:
    type: ClusterIP
    port: 80
    targetPort: 8086
    annotations: {}
    labels: {}
    portName: http
  serviceAccount:
    # Specifies whether a ServiceAccount should be created
    create: true
    # The name of the ServiceAccount to use.

    # If not set and create is true, a name is generated using the fullname template
    name:
    # Annotations for the ServiceAccount
    annotations: {}
  ingress:
    enabled: false
    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName

    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress

    # className: nginx
    tls: false
    # secretName: my-tls-cert # only needed if tls above is true or default certificate is not configured for Nginx

    hostname: influxdb.foobar.com
    annotations: {}
    # kubernetes.io/ingress.class: "nginx"

    # kubernetes.io/tls-acme: "true"
    path: /
  ## Pod disruption budget configuration

  ##
  pdb:
    ## Specifies whether a Pod disruption budget should be created

    ##
    create: true
    minAvailable: 1
    # maxUnavailable: 1
  iroute:
    enabled: true
    externalUrl: influxdb-centralized-gateway.liquidfaas.cloud
couchdb:
  # -- the initial number of nodes in the CouchDB cluster.
  clusterSize: 3
  # -- If allowAdminParty is enabled the cluster will start up without any database

  # administrator account; i.e., all users will be granted administrative

  # access. Otherwise, the system will look for a Secret called

  # <ReleaseName>-couchdb containing `adminUsername`, `adminPassword` and

  # `cookieAuthSecret` keys. See the `createAdminSecret` flag.

  # ref: https://kubernetes.io/docs/concepts/configuration/secret/
  allowAdminParty: false
  # Set it to true to automatically enable the cluster after installation.

  # It will create a post-install job that will send the {"action": "finish_cluster"}

  # message to CouchDB to finalize the cluster and add the defaultDatabases listed.

  # Note that this job needs service.enabled to be set to true and if you use adminHash,

  # a valid adminPassword in the secret. Also set the --wait flag when you install to

  # avoid first jobs failure (helm install --wait ...)
  autoSetup:
    enabled: false
    image:
      repository: curlimages/curl
      tag: latest
      pullPolicy: Always
    defaultDatabases:
      - _global_changes
  # -- If createAdminSecret is enabled a Secret called <ReleaseName>-couchdb will

  # be created containing auto-generated credentials. Users who prefer to set

  # these values themselves have a couple of options:

  #

  # 1) The `adminUsername`, `adminPassword`, `adminHash`, and `cookieAuthSecret`

  #    can be defined directly in the chart's values. Note that all of a chart's

  #    values are currently stored in plaintext in a ConfigMap in the tiller

  #    namespace.

  #

  # 2) This flag can be disabled and a Secret with the required keys can be

  #    created ahead of time.
  createAdminSecret: true
  adminUsername: admin
  adminPassword: admin
  # adminHash: -pbkdf2-this_is_not_necessarily_secure_either

  # cookieAuthSecret: neither_is_this

  ## When enabled, will deploy a networkpolicy that allows CouchDB pods to

  ## communicate with each other for clustering and ingress on port 5984
  networkPolicy:
    enabled: true
  ## Use an alternate scheduler, e.g. "stork".

  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/

  ##

  # schedulerName:

  # Use a service account
  serviceAccount:
    enabled: true
    create: true
  # name:

  # imagePullSecrets:

  # - name: myimagepullsecret

  # -- The storage volume used by each Pod in the StatefulSet. If a

  # persistentVolume is not enabled, the Pods will use `emptyDir` ephemeral

  # local storage. Setting the storageClass attribute to "-" disables dynamic

  # provisioning of Persistent Volumes; leaving it unset will invoke the default

  # provisioner.
  persistentVolume:
    enabled: true
    # NOTE: the number of existing claims must match the cluster size

    existingClaims: []
    annotations: {}
    accessModes:
      - ReadWriteOnce
    size: 10Gi
    # storageClass: "-"
  ## The CouchDB image
  image:
    repository: couchdb
    tag: 3.2.1
    pullPolicy: IfNotPresent
  ## Experimental integration with Lucene-powered fulltext search

  searchImage:
    repository: kocolosk/couchdb-search
    tag: 0.2.0
    pullPolicy: IfNotPresent
  # -- Flip this to flag to include the Search container in each Pod

  enableSearch: false
  initImage:
    repository: busybox
    tag: latest
    pullPolicy: Always
  ## CouchDB is happy to spin up cluster nodes in parallel, but if you encounter

  ## problems you can try setting podManagementPolicy to the StatefulSet default

  ## `OrderedReady`
  podManagementPolicy: Parallel
  ## To better tolerate Node failures, we can prevent Kubernetes scheduler from

  ## assigning more than one Pod of CouchDB StatefulSet per Node using podAntiAffinity.
  affinity: {}
  # podAntiAffinity:

  #   requiredDuringSchedulingIgnoredDuringExecution:

  #     - labelSelector:

  #         matchExpressions:

  #           - key: "app"

  #             operator: In

  #             values:

  #             - couchdb

  #       topologyKey: "kubernetes.io/hostname"

  ## To control how Pods are spread across your cluster among failure-domains such as regions,

  ## zones, nodes, and other user-defined topology domains use topologySpreadConstraints.
  topologySpreadConstraints: {}
  # topologySpreadConstraints:

  #   - maxSkew: 1

  #     topologyKey: "topology.kubernetes.io/zone"

  #     whenUnsatisfiable: ScheduleAnyway

  #     labelSelector:

  #       matchLabels:

  #         app: couchdb

  ## Optional pod annotations
  annotations: {}
  ## Optional tolerations

  tolerations: []
  ## A StatefulSet requires a headless Service to establish the stable network

  ## identities of the Pods, and that Service is created automatically by this

  ## chart without any additional configuration. The Service block below refers

  ## to a second Service that governs how clients connect to the CouchDB cluster.
  service:
    annotations: {}
    enabled: true
    type: ClusterIP
    externalPort: "5984"
    labels: {}
  ## An Ingress resource can provide name-based virtual hosting and TLS

  ## termination among other things for CouchDB deployments which are accessed

  ## from outside the Kubernetes cluster.

  ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
  ingress:
    enabled: false
    # className: nginx

    hosts:
      - chart-example.local
    path: /
    annotations: {}
    # kubernetes.io/ingress.class: nginx

    # kubernetes.io/tls-acme: "true"
    tls:
    # Secrets must be manually created in the namespace.

    # - secretName: chart-example-tls

    #   hosts:

    #     - chart-example.local
  ## Optional resource requests and limits for the CouchDB container

  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}
  # requests:

  #  cpu: 100m

  #  memory: 128Mi

  # limits:

  #  cpu: 56

  #  memory: 256Gi

  # -- erlangFlags is a map that is passed to the Erlang VM as flags using the

  # ERL_FLAGS env. The `name` flag is required to establish connectivity

  # between cluster nodes.

  # ref: http://erlang.org/doc/man/erl.html#init_flags
  erlangFlags:
    name: couchdb
    # Older versions of the official CouchDB image (anything prior to 3.2.1)
  # do not act on the COUCHDB_ERLANG_COOKIE environment variable, so if you

  # want to cluster these deployments it's necessary to pass in a cookie here

  # setcookie: make-something-up

  # -- couchdbConfig will override default CouchDB configuration settings.

  # The contents of this map are reformatted into a .ini file laid down

  # by a ConfigMap object.

  # ref: http://docs.couchdb.org/en/latest/config/index.html
  couchdbConfig:
    couchdb:
      # Unique identifier for this CouchDB server instance
      uuid: decafbaddecafbaddecafbaddecafbad
    # cluster:

    #   q: 8 # Create 8 shards for each database
    chttpd:
      bind_address: any
      # chttpd.require_valid_user disables all the anonymous requests to the port

      # 5984 when is set to true.
      require_valid_user: false
      # required to use Fauxton if chttpd.require_valid_user is set to true
  # httpd:

  #   WWW-Authenticate: "Basic realm=\"administrator\""

  # Kubernetes local cluster domain.

  # This is used to generate FQDNs for peers when joining the CouchDB cluster.
  dns:
    clusterDomainSuffix: cluster.local
  ## Configure liveness and readiness probe values

  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  livenessProbe:
    enabled: true
    failureThreshold: 1000
    initialDelaySeconds: 100
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 10000
  readinessProbe:
    enabled: true
    failureThreshold: 1000
    initialDelaySeconds: 100
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 10000
  # CouchDB 3.2.0 adds in a metrics endpoint on the path `/_node/_local/_prometheus`.

  # Optionally, a standalone, unauthenticated port can be exposed for these metrics.
  prometheusPort:
    enabled: false
    bind_address: "0.0.0.0"
    port: 17986
  # Configure arbitrary sidecar containers for CouchDB pods created by the

  # StatefulSet
  sidecars: {}
  # - name: foo

  #   image: "busybox"

  #   imagePullPolicy: IfNotPresent

  #   resources:

  #     requests:

  #       cpu: "0.1"

  #       memory: 10Mi

  #   command: ['echo "foo";']

  #   volumeMounts:

  #     - name: database-storage

  #       mountPath: /opt/couchdb/data/

  # Placement manager to annotate each document in the nodes DB with "zone" attribute

  # recording the zone where node has been scheduled

  # Ref: https://docs.couchdb.org/en/stable/cluster/sharding.html#specifying-database-placement
  placementConfig:
    enabled: false
    image:
      repository: caligrafix/couchdb-autoscaler-placement-manager
      tag: 0.1.0
  iroute:
    enabled: true
    gateway: couchdb-gateway.liquidfaas.cloud
mqtt-provider:
  provider:
    enabled: true
    providerPort: "3000"
    couchdbUsername: admin
    couchdbPassword: admin
    couchdbGateway: "couchdb-gateway.liquidfaas.cloud"
    couchdbExtPort: "443"
  openwhisk:
    apiHost: "38.242.158.232"